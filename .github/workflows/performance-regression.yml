name: Performance Regression Detection

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  performance-regression:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.10, 3.11]
        
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for baseline comparison
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark py-spy memory-profiler
        
    - name: Create performance baseline (if not exists)
      id: baseline
      run: |
        if [ ! -f "performance_baseline.json" ]; then
          echo "Creating initial performance baseline..."
          python scripts/run_baseline_profiling.py --output-dir baseline_results --quick
          cp baseline_results/baseline_results.json performance_baseline.json
          echo "baseline_created=true" >> $GITHUB_OUTPUT
        else
          echo "baseline_created=false" >> $GITHUB_OUTPUT
        fi
        
    - name: Run performance tests
      run: |
        # Run performance tests with benchmark comparison
        pytest tests/performance/ -v --benchmark-only --benchmark-compare --benchmark-compare-fail=mean:5% --benchmark-save=current
        
    - name: Run baseline profiling
      run: |
        python scripts/run_baseline_profiling.py --output-dir current_results --quick
        
    - name: Compare performance with baseline
      id: compare
      run: |
        if [ -f "performance_baseline.json" ] && [ -f "current_results/baseline_results.json" ]; then
          python -c "
        import json
        import sys
        
        # Load baseline and current results
        with open('performance_baseline.json', 'r') as f:
            baseline = json.load(f)
        with open('current_results/baseline_results.json', 'r') as f:
            current = json.load(f)
            
        # Compare benchmark results
        baseline_benchmarks = baseline.get('benchmark_results', {}).get('benchmarks', [])
        current_benchmarks = current.get('benchmark_results', {}).get('benchmarks', [])
        
        regressions = []
        improvements = []
        
        for current_bench in current_benchmarks:
            name = current_bench['name']
            current_time = current_bench['mean_time']
            
            # Find matching baseline
            baseline_bench = next((b for b in baseline_benchmarks if b['name'] == name), None)
            if baseline_bench:
                baseline_time = baseline_bench['mean_time']
                change_percent = ((current_time - baseline_time) / baseline_time) * 100
                
                if change_percent > 5:  # 5% regression threshold
                    regressions.append(f'{name}: {change_percent:.1f}% slower')
                elif change_percent < -5:  # 5% improvement threshold
                    improvements.append(f'{name}: {abs(change_percent):.1f}% faster')
        
        # Output results
        if regressions:
            print('PERFORMANCE_REGRESSIONS=' + '; '.join(regressions))
            print('regressions_detected=true')
        else:
            print('regressions_detected=false')
            
        if improvements:
            print('PERFORMANCE_IMPROVEMENTS=' + '; '.join(improvements))
            
        print(f'regression_count={len(regressions)}')
        print(f'improvement_count={len(improvements)}')
        " >> $GITHUB_OUTPUT
        else
          echo "regressions_detected=false" >> $GITHUB_OUTPUT
          echo "regression_count=0" >> $GITHUB_OUTPUT
        fi
        
    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-artifacts-${{ matrix.python-version }}
        path: |
          current_results/
          baseline_results/
          .benchmarks/
          
    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const regressions = '${{ steps.compare.outputs.regressions_detected }}' === 'true';
          const regressionCount = parseInt('${{ steps.compare.outputs.regression_count }}');
          const improvementCount = parseInt('${{ steps.compare.outputs.improvement_count }}');
          
          let comment = '## üöÄ Performance Test Results\n\n';
          
          if (regressions) {
            comment += '‚ö†Ô∏è **Performance regressions detected!**\n\n';
            comment += 'The following benchmarks show performance regressions:\n';
            comment += '```\n${{ steps.compare.outputs.PERFORMANCE_REGRESSIONS }}\n```\n\n';
            comment += 'Please review these changes and consider optimization.\n\n';
          } else {
            comment += '‚úÖ **No performance regressions detected**\n\n';
          }
          
          if (improvementCount > 0) {
            comment += 'üéâ **Performance improvements detected!**\n\n';
            comment += 'The following benchmarks show improvements:\n';
            comment += '```\n${{ steps.compare.outputs.PERFORMANCE_IMPROVEMENTS }}\n```\n\n';
          }
          
          comment += `**Summary:**\n`;
          comment += `- Regressions: ${regressionCount}\n`;
          comment += `- Improvements: ${improvementCount}\n`;
          comment += `- Python Version: ${{ matrix.python-version }}\n\n`;
          
          comment += 'üìä [View detailed performance artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
          
    - name: Fail on significant regressions
      if: steps.compare.outputs.regressions_detected == 'true'
      run: |
        echo "‚ùå Performance regressions detected. Build failed."
        echo "Regressions: ${{ steps.compare.outputs.PERFORMANCE_REGRESSIONS }}"
        exit 1
        
    - name: Update baseline on main branch
      if: github.ref == 'refs/heads/main' && steps.baseline.outputs.baseline_created == 'false'
      run: |
        # Update baseline with current results if no regressions
        if [ "${{ steps.compare.outputs.regressions_detected }}" == "false" ]; then
          cp current_results/baseline_results.json performance_baseline.json
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add performance_baseline.json
          git commit -m "Update performance baseline" || exit 0
          git push
        fi

  performance-stress-test:
    runs-on: ubuntu-latest
    needs: performance-regression
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        
    - name: Run stress tests
      run: |
        # Run stress tests for performance-critical components
        pytest tests/performance/ -v -m "stress" --timeout=300
        
    - name: Upload stress test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: stress-test-results
        path: |
          test_results/
          stress_test_artifacts/
